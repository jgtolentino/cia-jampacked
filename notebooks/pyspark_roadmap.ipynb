{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Complete Roadmap - Training Companion\n",
    "\n",
    "A comprehensive guide covering all essential PySpark concepts with hands-on examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RDD vs DataFrame vs Dataset\n",
    "\n",
    "Understanding the core abstractions in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark_Roadmap\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD - Resilient Distributed Dataset (Low-level API)\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "print(\"RDD Result:\", squared_rdd.collect())\n",
    "\n",
    "# DataFrame - Structured API with Schema\n",
    "data = [(1, \"Alice\", 25), (2, \"Bob\", 30), (3, \"Charlie\", 35)]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Dataset - Type-safe structured API (Not available in PySpark, only Scala/Java)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, lit, concat, upper, lower\n",
    "\n",
    "# Sample data\n",
    "employees = spark.createDataFrame([\n",
    "    (1, \"John\", \"Doe\", \"IT\", 75000, \"2020-01-15\"),\n",
    "    (2, \"Jane\", \"Smith\", \"HR\", 65000, \"2019-03-22\"),\n",
    "    (3, \"Bob\", \"Johnson\", \"IT\", 80000, \"2021-07-01\"),\n",
    "    (4, \"Alice\", \"Williams\", \"Finance\", 70000, None),\n",
    "    (5, \"Charlie\", \"Brown\", \"IT\", 72000, \"2020-11-30\")\n",
    "], [\"id\", \"first_name\", \"last_name\", \"department\", \"salary\", \"hire_date\"])\n",
    "\n",
    "# Select transformation\n",
    "print(\"Select columns:\")\n",
    "employees.select(\"first_name\", \"department\", \"salary\").show()\n",
    "\n",
    "# WithColumn transformation\n",
    "print(\"\\nAdd calculated column:\")\n",
    "employees_with_bonus = employees.withColumn(\n",
    "    \"bonus\",\n",
    "    when(col(\"department\") == \"IT\", col(\"salary\") * 0.15)\n",
    "    .otherwise(col(\"salary\") * 0.10)\n",
    ")\n",
    "employees_with_bonus.show()\n",
    "\n",
    "# Multiple transformations\n",
    "print(\"\\nChained transformations:\")\n",
    "result = employees \\\n",
    "    .withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))) \\\n",
    "    .withColumn(\"department_upper\", upper(col(\"department\"))) \\\n",
    "    .filter(col(\"salary\") > 70000) \\\n",
    "    .select(\"full_name\", \"department_upper\", \"salary\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQL Joins - All Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create departments table\n",
    "departments = spark.createDataFrame([\n",
    "    (\"IT\", \"Information Technology\", \"Building A\"),\n",
    "    (\"HR\", \"Human Resources\", \"Building B\"),\n",
    "    (\"Finance\", \"Financial Services\", \"Building C\"),\n",
    "    (\"Marketing\", \"Marketing & Sales\", \"Building D\")\n",
    "], [\"dept_code\", \"dept_name\", \"location\"])\n",
    "\n",
    "# Inner Join\n",
    "print(\"Inner Join:\")\n",
    "inner_join = employees.join(\n",
    "    departments,\n",
    "    employees.department == departments.dept_code,\n",
    "    \"inner\"\n",
    ").select(\"first_name\", \"last_name\", \"dept_name\", \"location\")\n",
    "inner_join.show()\n",
    "\n",
    "# Left Outer Join\n",
    "print(\"\\nLeft Outer Join:\")\n",
    "left_join = employees.join(\n",
    "    departments,\n",
    "    employees.department == departments.dept_code,\n",
    "    \"left\"\n",
    ").select(\"first_name\", \"department\", \"dept_name\")\n",
    "left_join.show()\n",
    "\n",
    "# Right Outer Join\n",
    "print(\"\\nRight Outer Join:\")\n",
    "right_join = employees.join(\n",
    "    departments,\n",
    "    employees.department == departments.dept_code,\n",
    "    \"right\"\n",
    ").select(\"first_name\", \"dept_code\", \"dept_name\")\n",
    "right_join.show()\n",
    "\n",
    "# Full Outer Join\n",
    "print(\"\\nFull Outer Join:\")\n",
    "full_join = employees.join(\n",
    "    departments,\n",
    "    employees.department == departments.dept_code,\n",
    "    \"outer\"\n",
    ").select(\"first_name\", \"department\", \"dept_code\", \"dept_name\")\n",
    "full_join.show()\n",
    "\n",
    "# Cross Join\n",
    "print(\"\\nCross Join (Cartesian Product):\")\n",
    "small_df1 = spark.createDataFrame([(1, \"A\"), (2, \"B\")], [\"id\", \"value\"])\n",
    "small_df2 = spark.createDataFrame([(\"X\",), (\"Y\",)], [\"letter\"])\n",
    "cross_join = small_df1.crossJoin(small_df2)\n",
    "cross_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Missing Data - fillna, dropna, cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType, StringType\n",
    "from datetime import datetime\n",
    "\n",
    "# Create DataFrame with null values\n",
    "null_data = spark.createDataFrame([\n",
    "    (1, \"John\", 25, 50000.0, \"2021-01-15\"),\n",
    "    (2, None, 30, 60000.0, \"2021-02-20\"),\n",
    "    (3, \"Jane\", None, 55000.0, None),\n",
    "    (4, \"Bob\", 35, None, \"2021-04-10\"),\n",
    "    (5, \"Alice\", 28, 52000.0, None)\n",
    "], [\"id\", \"name\", \"age\", \"salary\", \"join_date\"])\n",
    "\n",
    "print(\"Original DataFrame with nulls:\")\n",
    "null_data.show()\n",
    "\n",
    "# dropna - Drop rows with null values\n",
    "print(\"\\nDrop rows with any null:\")\n",
    "null_data.dropna().show()\n",
    "\n",
    "print(\"\\nDrop rows with null in specific columns:\")\n",
    "null_data.dropna(subset=[\"name\", \"age\"]).show()\n",
    "\n",
    "# fillna - Fill null values\n",
    "print(\"\\nFill all nulls with defaults:\")\n",
    "filled_df = null_data.fillna({\n",
    "    \"name\": \"Unknown\",\n",
    "    \"age\": 0,\n",
    "    \"salary\": 50000.0,\n",
    "    \"join_date\": \"2021-01-01\"\n",
    "})\n",
    "filled_df.show()\n",
    "\n",
    "# Cast - Type conversion\n",
    "print(\"\\nType casting examples:\")\n",
    "casted_df = null_data \\\n",
    "    .withColumn(\"age_double\", col(\"age\").cast(DoubleType())) \\\n",
    "    .withColumn(\"salary_int\", col(\"salary\").cast(IntegerType())) \\\n",
    "    .withColumn(\"id_string\", col(\"id\").cast(StringType()))\n",
    "\n",
    "casted_df.select(\"id\", \"id_string\", \"age\", \"age_double\", \"salary\", \"salary_int\").show()\n",
    "casted_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. UDF & Pandas UDF (Vectorized UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, pandas_udf\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "import pandas as pd\n",
    "\n",
    "# Basic UDF (Row-by-row processing)\n",
    "def categorize_age(age):\n",
    "    if age < 25:\n",
    "        return \"Young\"\n",
    "    elif age < 35:\n",
    "        return \"Middle\"\n",
    "    else:\n",
    "        return \"Senior\"\n",
    "\n",
    "# Register basic UDF\n",
    "categorize_age_udf = udf(categorize_age, StringType())\n",
    "\n",
    "# Apply basic UDF\n",
    "print(\"Basic UDF Example:\")\n",
    "employees_categorized = employees.withColumn(\n",
    "    \"age_category\",\n",
    "    categorize_age_udf(col(\"salary\") / 2500)  # Dummy age calculation\n",
    ")\n",
    "employees_categorized.select(\"first_name\", \"salary\", \"age_category\").show()\n",
    "\n",
    "# Pandas UDF (Vectorized - Much faster!)\n",
    "@pandas_udf(returnType=StringType())\n",
    "def categorize_salary_pandas(salary: pd.Series) -> pd.Series:\n",
    "    return salary.apply(\n",
    "        lambda x: \"High\" if x > 75000 else \"Medium\" if x > 65000 else \"Low\"\n",
    "    )\n",
    "\n",
    "# Apply Pandas UDF\n",
    "print(\"\\nPandas UDF Example:\")\n",
    "employees_salary_cat = employees.withColumn(\n",
    "    \"salary_category\",\n",
    "    categorize_salary_pandas(col(\"salary\"))\n",
    ")\n",
    "employees_salary_cat.select(\"first_name\", \"salary\", \"salary_category\").show()\n",
    "\n",
    "# Grouped Map Pandas UDF\n",
    "@pandas_udf(\"id long, first_name string, salary double, avg_dept_salary double\", \"grouped_map\")\n",
    "def normalize_salaries(pdf):\n",
    "    avg_salary = pdf['salary'].mean()\n",
    "    pdf['avg_dept_salary'] = avg_salary\n",
    "    return pdf[['id', 'first_name', 'salary', 'avg_dept_salary']]\n",
    "\n",
    "print(\"\\nGrouped Map Pandas UDF:\")\n",
    "normalized = employees.groupby(\"department\").apply(normalize_salaries)\n",
    "normalized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Repartition vs Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset\n",
    "large_data = spark.range(0, 1000000).withColumn(\"value\", col(\"id\") * 2)\n",
    "\n",
    "print(f\"Initial partitions: {large_data.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition - Full shuffle, can increase or decrease partitions\n",
    "repartitioned = large_data.repartition(10)\n",
    "print(f\"After repartition(10): {repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartition by column - Useful for downstream operations\n",
    "employees_repartitioned = employees.repartition(\"department\")\n",
    "print(f\"\\nEmployees partitioned by department: {employees_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce - No shuffle, only decrease partitions\n",
    "coalesced = repartitioned.coalesce(5)\n",
    "print(f\"After coalesce(5): {coalesced.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Performance comparison\n",
    "import time\n",
    "\n",
    "# Repartition timing\n",
    "start = time.time()\n",
    "large_data.repartition(20).count()\n",
    "repartition_time = time.time() - start\n",
    "\n",
    "# Coalesce timing\n",
    "start = time.time()\n",
    "large_data.coalesce(2).count()\n",
    "coalesce_time = time.time() - start\n",
    "\n",
    "print(f\"\\nRepartition time: {repartition_time:.2f}s\")\n",
    "print(f\"Coalesce time: {coalesce_time:.2f}s\")\n",
    "print(f\"Coalesce is {repartition_time/coalesce_time:.2f}x faster (no shuffle!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explain Plans & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for join analysis\n",
    "large_table = spark.range(0, 1000000).withColumn(\"value\", col(\"id\") * 10)\n",
    "small_table = spark.range(0, 100).withColumn(\"multiplier\", col(\"id\") * 2)\n",
    "\n",
    "# Regular join\n",
    "regular_join = large_table.join(small_table, large_table.id == small_table.id)\n",
    "\n",
    "print(\"Regular Join Explain Plan:\")\n",
    "regular_join.explain(True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Broadcast join\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "broadcast_join = large_table.join(broadcast(small_table), large_table.id == small_table.id)\n",
    "\n",
    "print(\"Broadcast Join Explain Plan:\")\n",
    "broadcast_join.explain(True)\n",
    "\n",
    "# Analyzing shuffle stages\n",
    "complex_query = employees \\\n",
    "    .groupBy(\"department\") \\\n",
    "    .agg({\"salary\": \"avg\"}) \\\n",
    "    .join(departments, col(\"department\") == col(\"dept_code\")) \\\n",
    "    .orderBy(\"avg(salary)\", ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Complex Query with Multiple Stages:\")\n",
    "complex_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Broadcast Join Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure broadcast threshold\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "\n",
    "# Small lookup table\n",
    "lookup_table = spark.createDataFrame([\n",
    "    (\"IT\", 1.15),\n",
    "    (\"HR\", 1.10),\n",
    "    (\"Finance\", 1.12),\n",
    "    (\"Marketing\", 1.08)\n",
    "], [\"dept\", \"bonus_multiplier\"])\n",
    "\n",
    "# Method 1: Automatic broadcast (if table is small enough)\n",
    "auto_broadcast = employees.join(\n",
    "    lookup_table,\n",
    "    employees.department == lookup_table.dept\n",
    ")\n",
    "\n",
    "print(\"Automatic Broadcast Join:\")\n",
    "auto_broadcast.explain()\n",
    "\n",
    "# Method 2: Force broadcast hint\n",
    "forced_broadcast = employees.join(\n",
    "    broadcast(lookup_table),\n",
    "    employees.department == lookup_table.dept\n",
    ").withColumn(\n",
    "    \"bonus_amount\",\n",
    "    col(\"salary\") * col(\"bonus_multiplier\")\n",
    ")\n",
    "\n",
    "print(\"\\nForced Broadcast Join with Calculation:\")\n",
    "forced_broadcast.select(\"first_name\", \"salary\", \"bonus_multiplier\", \"bonus_amount\").show()\n",
    "\n",
    "# Performance comparison\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(\"Regular join stages:\", regular_join.rdd.getNumPartitions())\n",
    "print(\"Broadcast join stages:\", broadcast_join.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. File I/O - Parquet, ORC, Avro, JSON, CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./spark_output\"\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "# Sample data for file operations\n",
    "file_data = employees_with_bonus.select(\"id\", \"first_name\", \"last_name\", \"department\", \"salary\", \"bonus\")\n",
    "\n",
    "# 1. Parquet (Columnar, compressed, efficient)\n",
    "parquet_path = f\"{output_dir}/employees.parquet\"\n",
    "file_data.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "print(\"Parquet file written\")\n",
    "\n",
    "# Read Parquet\n",
    "parquet_df = spark.read.parquet(parquet_path)\n",
    "print(\"\\nParquet Schema:\")\n",
    "parquet_df.printSchema()\n",
    "\n",
    "# 2. ORC (Optimized Row Columnar)\n",
    "orc_path = f\"{output_dir}/employees.orc\"\n",
    "file_data.write.mode(\"overwrite\").orc(orc_path)\n",
    "orc_df = spark.read.orc(orc_path)\n",
    "print(\"\\nORC file written and read\")\n",
    "\n",
    "# 3. JSON\n",
    "json_path = f\"{output_dir}/employees.json\"\n",
    "file_data.write.mode(\"overwrite\").json(json_path)\n",
    "json_df = spark.read.json(json_path)\n",
    "print(\"\\nJSON file written and read\")\n",
    "\n",
    "# 4. CSV with options\n",
    "csv_path = f\"{output_dir}/employees.csv\"\n",
    "file_data.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .csv(csv_path)\n",
    "\n",
    "csv_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(csv_path)\n",
    "print(\"\\nCSV file written and read\")\n",
    "\n",
    "# 5. Avro (requires spark-avro package)\n",
    "# avro_path = f\"{output_dir}/employees.avro\"\n",
    "# file_data.write.mode(\"overwrite\").format(\"avro\").save(avro_path)\n",
    "\n",
    "# Compare file sizes\n",
    "print(\"\\nFile Size Comparison:\")\n",
    "for format_name, path in [(\"Parquet\", parquet_path), (\"ORC\", orc_path), \n",
    "                          (\"JSON\", json_path), (\"CSV\", csv_path)]:\n",
    "    size = sum(os.path.getsize(os.path.join(path, f)) \n",
    "               for f in os.listdir(path) if f.endswith(format_name.lower()))\n",
    "    print(f\"{format_name}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. JDBC Connector - Database Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/testdb\"\n",
    "connection_properties = {\n",
    "    \"user\": \"username\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Example read from database (commented out - requires actual DB)\n",
    "\"\"\"\n",
    "# Read entire table\n",
    "df_from_db = spark.read \\\n",
    "    .jdbc(url=jdbc_url, \n",
    "          table=\"employees\",\n",
    "          properties=connection_properties)\n",
    "\n",
    "# Read with custom query\n",
    "query = \"(SELECT * FROM employees WHERE salary > 50000) AS high_earners\"\n",
    "df_custom = spark.read \\\n",
    "    .jdbc(url=jdbc_url,\n",
    "          table=query,\n",
    "          properties=connection_properties)\n",
    "\n",
    "# Parallel read with partitioning\n",
    "df_parallel = spark.read \\\n",
    "    .jdbc(url=jdbc_url,\n",
    "          table=\"large_table\",\n",
    "          column=\"id\",\n",
    "          lowerBound=1,\n",
    "          upperBound=1000000,\n",
    "          numPartitions=10,\n",
    "          properties=connection_properties)\n",
    "\n",
    "# Write to database\n",
    "employees_with_bonus.write \\\n",
    "    .mode(\"append\") \\\n",
    "    .jdbc(url=jdbc_url,\n",
    "          table=\"employee_bonuses\",\n",
    "          properties=connection_properties)\n",
    "\"\"\"\n",
    "\n",
    "# Simulated JDBC operations example\n",
    "print(\"JDBC Configuration Example:\")\n",
    "print(f\"URL: {jdbc_url}\")\n",
    "print(f\"Properties: {connection_properties}\")\n",
    "print(\"\\nFor parallel reads, use:\")\n",
    "print(\"- column: partition column (should be numeric)\")\n",
    "print(\"- lowerBound/upperBound: range of partition column\")\n",
    "print(\"- numPartitions: number of parallel connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache and persist\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Cache in memory\n",
    "cached_df = employees_with_bonus.cache()\n",
    "print(\"DataFrame cached in memory\")\n",
    "\n",
    "# Persist with different storage levels\n",
    "persisted_df = employees_with_bonus.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(\"DataFrame persisted to memory and disk\")\n",
    "\n",
    "# Unpersist when done\n",
    "cached_df.unpersist()\n",
    "persisted_df.unpersist()\n",
    "\n",
    "# Adaptive Query Execution (AQE)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "# Data skew handling example\n",
    "skewed_data = spark.createDataFrame([\n",
    "    (\"A\", i) for i in range(1000)\n",
    "] + [\n",
    "    (\"B\", i) for i in range(10)\n",
    "], [\"key\", \"value\"])\n",
    "\n",
    "# This join would have skew issues\n",
    "skewed_join = skewed_data.alias(\"left\").join(\n",
    "    skewed_data.alias(\"right\"),\n",
    "    col(\"left.key\") == col(\"right.key\")\n",
    ")\n",
    "\n",
    "print(\"\\nAdaptive Query Execution will handle skew automatically\")\n",
    "print(f\"Current AQE setting: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "\n",
    "# Partition pruning with partitioned tables\n",
    "partitioned_path = f\"{output_dir}/partitioned_employees\"\n",
    "employees_with_bonus.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"department\") \\\n",
    "    .parquet(partitioned_path)\n",
    "\n",
    "# Reading with partition filter (much faster!)\n",
    "it_employees = spark.read.parquet(partitioned_path).filter(col(\"department\") == \"IT\")\n",
    "print(\"\\nPartition pruning will only read IT department files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, lead, lag, sum as spark_sum\n",
    "\n",
    "# Window specifications\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "windowSpecRange = Window.partitionBy(\"department\").orderBy(\"salary\").rangeBetween(-10000, 10000)\n",
    "\n",
    "# Apply window functions\n",
    "windowed_df = employees \\\n",
    "    .withColumn(\"row_num\", row_number().over(windowSpec)) \\\n",
    "    .withColumn(\"rank\", rank().over(windowSpec)) \\\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n",
    "    .withColumn(\"next_salary\", lead(\"salary\", 1).over(windowSpec)) \\\n",
    "    .withColumn(\"prev_salary\", lag(\"salary\", 1).over(windowSpec)) \\\n",
    "    .withColumn(\"salary_range_sum\", spark_sum(\"salary\").over(windowSpecRange))\n",
    "\n",
    "print(\"Window Functions Example:\")\n",
    "windowed_df.select(\n",
    "    \"first_name\", \"department\", \"salary\", \n",
    "    \"row_num\", \"rank\", \"dense_rank\",\n",
    "    \"prev_salary\", \"next_salary\"\n",
    ").show()\n",
    "\n",
    "# Running totals\n",
    "runningTotalWindow = Window.partitionBy(\"department\").orderBy(\"id\").rowsBetween(\n",
    "    Window.unboundedPreceding, Window.currentRow\n",
    ")\n",
    "\n",
    "running_totals = employees \\\n",
    "    .withColumn(\"running_total\", spark_sum(\"salary\").over(runningTotalWindow))\n",
    "\n",
    "print(\"\\nRunning Totals:\")\n",
    "running_totals.select(\"first_name\", \"department\", \"salary\", \"running_total\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()\n",
    "\n",
    "# Clean up output directory\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "    print(\"Cleaned up output directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}